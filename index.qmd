---
title: "The Effects of Monetary Policy Shocks on Stock Price Volatility: Evidence from the Australian Economy"
author: "Inhye Kang"

execute:
  echo: false
  
bibliography: references.bib
csl: apa.csl
---

> **Abstract.** This research project aims to measure the effects of monetary policy shocks on stock price volatility using the Bayesian Structural Vector Autoregressive Model in the Australian economy from 1990 to 2023.
>
> **Keywords.** Bayesian Structural VARs, Monetary policy shocks, Stock price volatility, Impulse response function

# 1. Introduction

This research project aims to measure the effects of monetary policy shocks on stock price volatility using a Bayesian Structural Vector Autoregressive Model in the Australian economy from 1990 to 2023. The question addressed in this study is: How does an increase in the cash rate target affect stock price volatility in the Australian market?

The behaviour and decisions of stakeholders--including investors, businesses, and financial institutions--in financial markets are primarily affected by stock price volatility. As such, the effect of change in stock prices plays a large role in Australian economy. This study aims to measure the monetary policy shocks on stock price volatility using the Bayesian Structural Vector Autoregression model, focusing on the Australian economy.

# 2. Data

The study includes data from the Reserve Bank of Australia (RBA), the Australian Bureau of Statistics (ABS), and Yahoo Finance from 1990 to 2023, adjusted quarterly from 1990 Q1 to 2023 Q4, including 136 observations. The variables selected for our analysis include:

|    **GDP (GDP)**: real GDP, expressed as per billion AUD
|    **Interest rates (ICR)**: cash rate target, expressed as a percentage
|    **Consumer price index (CPI)**: All groups Consumer Price Index, expressed as an index number
|    **Exchange rates (EXR)**: exchange rates from AUD to USD, expressed in USD
|    **Stock prices (STP)**: stock prices using the adjusted closing price of the All Ordinaries Index, expressed in AUD
|    **Stock price volatility (VOL)**: the log of bipower variation calculated using the log of the sum of the
|    multiplication of consecutive absolute logarithmic returns of stock prices (STP) multiplied by normalisation
|    factor $\frac{\pi}{2}$ where $log(\frac{\pi}{2} \sum_{t=2}^{T} |r_{t}| \cdot |r_{t-1}|)$

[<span style="color: #696969;">Table 1](#table1-data-source) shows the details of the data source.
```{r sources}
source <- data.frame(
  c('GDP', 'Interest rates', 'Consumer Price Index', 'Exchange rates', 'Stock prices'),  
  c('GGDPCVGDP', 'FIRMMCRTD', 'A2325846C', 'FXRUSD', '^AORD'),
  c('RBA', 'RBA', 'ABS', 'RBA', 'Yahoo Finance'),
  stringsAsFactors=FALSE  
)
colnames(source) <- c("Variable", "Code", "Data source")

knitr::kable(source)
```
<div id="table1" style="text-align: center; color: #696969;">
###### Table 1: Data source
</div>

```{r global options}
knitr::opts_chunk$set(echo=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(fig.align="center")
```

```{r download the variables}
# Download seasonally adjusted quarterly Real GDP ($ million AUD)
gdp_download <- readrba::read_rba(series_id = "GGDPCVGDP")   # Real GDP
gdp_tmp <- xts::xts(gdp_download$value, gdp_download$date)
gdp_tmp <- window(gdp_tmp, start=as.Date("1990-01-01"), end=as.Date("2023-12-31"))
gdp_tmp <- gdp_tmp/1000   # Real GDP ($ billion AUD)

# Download daily averaged interest rates in percentage (%)
icr_download <- readrba::read_rba(series_id = "FIRMMCRTD")   # Cash rate target (%)
icr_tmp <- xts::xts(icr_download$value, icr_download$date)
icr_tmp <- window(icr_tmp, start=as.Date("1990-01-01"), end=as.Date("2023-12-31"))

# Download quarterly Consumer Price Index
cpi_download <- readabs::read_abs(series_id = "A2325846C")   # CPI
cpi_tmp <- xts::xts(cpi_download$value, cpi_download$date)
cpi_tmp <- window(cpi_tmp, start=as.Date("1990-01-01"), end=as.Date("2023-12-31"))

# Download daily exchange rates (AUD/USD Exchange Rates) in USD
exr_download <- readrba::read_rba(series_id = "FXRUSD")   # Exchange rates of 1 AUD to USD
exr_tmp <- xts::xts(exr_download$value, exr_download$date)
exr_tmp <- window(exr_tmp, start=as.Date("1990-01-01"), end=as.Date("2023-12-31"))

# Download daily stock prices (^AORD) in AUD
stp_download <- tidyquant::tq_get("^AORD", from="1990-01-01", to="2023-12-31")   # Stock price (AUD)
stp_tmp <- xts::xts(stp_download$adjusted, stp_download$date)
stp_tmp <- window(stp_tmp, start=as.Date("1990-01-01"), end=as.Date("2023-12-31"))

# Calculate bipower variation of daily stock prices ^AORD
log_stp <- diff(log(stp_tmp))  # Log returns
log_stp <- na.omit(log_stp)
abs_stp <- abs(log_stp)  # Absolute returns
bpv_daily <- (pi/2)*abs_stp[1:(length(abs_stp)-1)]*abs_stp[2:length(abs_stp)]   # Daily bipower variation
```

```{r convert to quarterly system}
# Convert variables into quarterly system
library(zoo)
GDP <- aggregate(gdp_tmp, as.yearqtr, function(x) mean(x, na.rm=TRUE))  # real GDP ($ M)
ICR <- aggregate(icr_tmp, as.yearqtr, function(x) mean(x, na.rm=TRUE))  # cash rate target (%)
CPI <- aggregate(cpi_tmp, as.yearqtr, function(x) mean(x, na.rm=TRUE)) # consumer price index
EXR <- aggregate(exr_tmp, as.yearqtr, function(x) mean(x, na.rm=TRUE))  # exchange rates (1 AUD/USD)
STP <- aggregate(stp_tmp, as.yearqtr, function(x) mean(x, na.rm=TRUE))  # stock prices (AUD)

quarters <- xts::endpoints(bpv_daily, on="quarters")
VOL <- xts::period.apply(bpv_daily, INDEX=quarters, FUN=sum)
vol <- log(VOL)
index(vol) <- as.yearqtr(index(vol), format="%Y Q%q")  # log stock price volatility
```

GDP, interest rates, consumer price index, exchange rates, stock prices and stock price volatility are plotted in [<span style="color: #696969;">Figure 1](#figure1). GDP exhibits an upward trend except for a trough during COVID-19. Interest rates depict a gradual downward trend; however, from 2022 Q4, there has been a significant increase in interest rates. There is an overall upward trend of stock prices, however, with the exception during the Global Financial Crisis where the stock prices has shown great volatility by increasing and decreasing in large amount.

```{r time series plots, fig.width=7, fig.height=6.5}
library(ggplot2)
library(gridExtra)

plotGDP <- ggplot2::ggplot(data=GDP, aes(x=time(GDP), y=as.vector(GDP))) +
  geom_line(color="darkblue", linewidth=0.5) + 
  scale_y_continuous() + 
  labs(title="GDP ($ billion AUD)", x="", y="") +
  theme_classic() +
  theme(plot.title=element_text(hjust=0.5), axis.text.x=element_text(angle=45, hjust=1))

plotICR <- ggplot2::ggplot(data=ICR, aes(x=time(ICR), y=as.vector(ICR))) +
  geom_line(color="darkblue", linewidth=0.5) + 
  geom_hline(yintercept=0, linetype="solid", color="darkgrey", linewidth=0.5) +
  scale_y_continuous() + 
  labs(title="Interest rates (%)", x="", y="") +
  theme_classic() +
  theme(plot.title=element_text(hjust=0.5), axis.text.x=element_text(angle=45, hjust=1))

plotCPI <- ggplot2::ggplot(data=CPI, aes(x=time(CPI), y=as.vector(CPI))) +
  geom_line(color="darkblue", linewidth=0.5) + 
  scale_y_continuous() + 
  labs(title="Consumer price index", x="", y="") +
  theme_classic() +
  theme(plot.title=element_text(hjust=0.5), axis.text.x=element_text(angle=45, hjust=1))

plotEXR <- ggplot2::ggplot(data=EXR, aes(x=time(EXR), y=as.vector(EXR))) +
  geom_line(color="darkblue", linewidth=0.5) + 
  geom_hline(yintercept=1, linetype="solid", color="darkgrey", linewidth=0.5) +
  scale_y_continuous() + 
  labs(title ="Exchange rates (AUD/USD)", x="", y="") +
  theme_classic() +
  theme(plot.title=element_text(hjust=0.5), axis.text.x=element_text(angle=45, hjust=1))

plotSTP <- ggplot2::ggplot(data=STP, aes(x=time(STP), y=as.vector(STP))) +
  geom_line(color="darkblue", linewidth=0.5) + 
  scale_y_continuous() + 
  labs(title="Stock prices ($ AUD)", x="", y="") +
  theme_classic() +
  theme(plot.title=element_text(hjust=0.5), axis.text.x=element_text(angle=45, hjust=1))

plotvol <- ggplot2::ggplot(data=vol, aes(x=time(vol), y=as.vector(vol))) +
  geom_line(color="darkblue", linewidth=0.5) + 
  scale_y_continuous() + 
  labs(title="Stock price volatility", x="", y="") +
  theme_classic() +
  theme(plot.title=element_text(hjust=0.5), axis.text.x=element_text(angle=45, hjust=1))

grid.arrange(plotGDP, plotICR, plotCPI, plotEXR, plotSTP, plotvol, nrow=3, ncol=2)
```
<div id="figure1" style="text-align: center; color: #696969;">
###### Figure 1: Time series plots of the variables
</div>

[<span style="color: #696969;">Table 2](#table2) demonstrates the descriptive statistics of the variables from 1990 Q1 to 2023 Q4.
```{r descriptive statistics}
# Original dataset
original_variables <- merge(GDP, ICR, CPI, EXR, STP, vol)
colnames(original_variables) <- c("GDP", "Interest rates", "Consumer price index", "Exchange rates", "Stock prices", "Stock price volatility")

# Summary statistics
summary_stats <- sapply(original_variables, function(x) {
  c(Mean = mean(x, na.rm=TRUE),
    SD = sd(x, na.rm=TRUE),
    Min = min(x, na.rm=TRUE),
    Max = max(x, na.rm=TRUE),
    n = sum(!is.na(x)))
})

units <- c("GDP"="$ billion AUD ", "Interest rates"="Percentage", "Consumer price index"="Index", "Exchange rates"="$ USD", "Stock prices"="$ AUD", "Stock price volatility"=" ")

summary_df <- as.data.frame(t(summary_stats))
summary_df$Variable <- rownames(summary_df)
summary_df$Unit <- units[summary_df$Variable]
summary_df <- summary_df[, c("Variable", "Unit", "Mean", "SD", "Min", "Max", "n")]
rownames(summary_df) <- NULL

knitr::kable(summary_df, digits=3)
```
<div id="table2" style="text-align: center; color: #696969;">
###### Table 2: Summary of descriptive statistics
</div>

In our model, GDP, consumer price index, stock prices are transformed into a logarithmic scale. Stock price volatility is the log of bipower variation.
```{r convert to log scale}
logGDP <- log(GDP)
logCPI <- log(CPI)
logSTP <- log(STP)
```

```{r final dataset}
# Final dataset
variables <- merge(logGDP, ICR, logCPI, EXR, logSTP, vol)
colnames(variables) <- c("GDP", "Interest rates", "Consumer price index", "Exchange rates", "Stock prices", "Stock price volatility")
```

[<span style="color: #696969;">Figure 2](#figure2) shows the variables... stock price volatility is log of log stock price..
```{r log difference plots,  fig.width=7, fig.height=6.5}
plotlogGDP <- ggplot2::ggplot(data=logGDP, aes(x=time(logGDP), y=as.vector(logGDP))) +
  geom_line(color="darkblue", linewidth=0.5) + 
  labs(title="GDP", x="", y="") +
  theme_classic() +
  theme(plot.title=element_text(hjust=0.5), axis.text.x=element_text(angle=45, hjust=1))

plotICR <- ggplot2::ggplot(data=ICR, aes(x=time(ICR), y=as.vector(ICR))) +
  geom_line(color="darkblue", linewidth=0.5) + 
  geom_hline(yintercept=0, linetype="solid", color="darkgrey", linewidth=0.5) +
  labs(title="Interest rates (%)", x="", y="") +
  theme_classic() +
  theme(plot.title=element_text(hjust=0.5), axis.text.x=element_text(angle=45, hjust=1))

plotlogCPI <- ggplot2::ggplot(data=logCPI, aes(x=time(logCPI), y=as.vector(logCPI))) +
  geom_line(color="darkblue", linewidth=0.5) + 
  labs(title="Consumer price index", x="", y="") +
  theme_classic() +
  theme(plot.title=element_text(hjust=0.5), axis.text.x=element_text(angle=45, hjust=1))

plotEXR <- ggplot2::ggplot(data=EXR, aes(x=time(EXR), y=as.vector(EXR))) +
  geom_line(color="darkblue", linewidth=0.5) + 
  geom_hline(yintercept=1, linetype="solid", color="darkgrey", linewidth=0.5) +
  labs(title="Exchange rates (AUD/USD)", x="", y="") +
  theme_classic() +
  theme(plot.title=element_text(hjust=0.5), axis.text.x=element_text(angle=45, hjust=1))

plotlogSTP <- ggplot2::ggplot(data=logSTP, aes(x=time(logSTP), y=as.vector(logSTP))) +
  geom_line(color="darkblue", linewidth=0.5) + 
  labs(title="Stock prices", x="", y="") +
  theme_classic() +
  theme(plot.title=element_text(hjust=0.5), axis.text.x=element_text(angle=45, hjust=1))

grid.arrange(plotlogGDP, plotICR, plotlogCPI, plotEXR, plotlogSTP, plotvol, nrow=3, ncol=2)
```
<div id="figure2" style="text-align: center; color: #696969;">
###### Figure 2: Plot for the data in logarithmic scale
</div>


## 2.1 Diagnostic Tests

### 2.1.1 Autocorrelation/Partial autocorrelation Function Plots

The autocorrelation test is used to identify the presence of serial correlation between a variable's current value and its lagged value, indicating that past values influence the current value.

The autocorrelation function (ACF) plots in [<span style="color: #696969;">Figure 3](#figure3) shows that all the variables except for stock price volatility have non-zero autocorrelation for at least 20 lags, implying that only stock price volatility is a stationary series and the other variables are highly persistent.

```{r ACF, fig.width=7.5, fig.height=7}
library(forecast)
par(mfrow=c(3, 2))
for (i in 1:ncol(variables)){
  Acf(variables[, i], main=colnames(variables)[i])
}
```
<div id="figure3" style="text-align: center; color: #696969;">
###### Figure 3: Plots of autocorrelation functions
</div>


The partial autocorrelation function (PACF) plots in [<span style="color: #696969;">Figure 4](#figure4) shows that the partial autocorrelation for all the variables are significant at the first lag. The partial autocorrelation for exchange rates is also significant at 2.

```{r PACF, fig.width=7.5, fig.height=7}
par(mfrow=c(3, 2))
for (i in 1:ncol(variables)){
  Pacf(variables[, i], main=colnames(variables)[i])
}
```
<div id="figure4" style="text-align: center; color: #696969;">
###### Figure 4: Plots of partial autocorrelation functions
</div>

### 2.1.2 Unit Root Test

#### Augmented Dickey-Fuller Test

The augmented Dickey-Fuller test of the null hypothesis of unit root nonstationarity was performed to test the presence of the unit root.

[<span style="color: #696969;">Table 3](#table3) shows that the null hypothesis was not rejected at the 1% significance level for all the variables but not for stock price volatility, implying that all the variables except for stock price volatility are nonstationary series. However, stock price volatility is the log of bipower variation and took logarithmic scale twice,

```{r ADF on the level}
library(urca)

adf <- as.data.frame(matrix(nrow=ncol(variables), ncol=4, NA))
colnames(adf) <- c("Variable", "Test statistic", "Critical value", "Stationarity")
adf$Variable <- colnames(variables)

for (i in 1: ncol(variables)){
  #lag_order            <- trunc((length(variables[, i]) - 1)^(1/3))
  adf_test1             <- ur.df(variables[, i], type="trend", lags=trunc((length(variables[, i]) - 1)^(1/3)), selectlags="AIC")
  adf_test2             <- ur.df(variables[, i], type="drift", lags=trunc((length(variables[, i]) - 1)^(1/3)), selectlags="AIC")
  if (colnames(variables)[i] %in% c("GDP", "Consumer price index", "Stock prices")){
    test_statistic      <- adf_test1@teststat["statistic", "tau3"]
    critical_value_1pct <- adf_test1@cval["tau3", "1pct"]
  } else {
    test_statistic      <- adf_test2@teststat["statistic", "tau2"]
    critical_value_1pct <- adf_test2@cval["tau2", "1pct"]
  }
  adf[i, "Test statistic"] <- round(as.numeric(test_statistic), 3)
  adf[i, "Critical value"] <- round(as.numeric(critical_value_1pct), 3)
  adf[i, "Stationarity"]   <- ifelse(test_statistic < critical_value_1pct, "Yes", "No")
  #adf[i, "Lag order"]     <- lag_order
}

knitr::kable(adf)
```
<div id="table3" style="text-align: center; color: #696969;">
###### Table 3: Augmented Dickey-Fuller test results
</div>

[<span style="color: #696969;">Table 4](#table4) shows that the Augmented Dickey-Fuller test results on the first difference of the variables. We find that all the variables are unit root stationary at the 1% significance level, and conclude that all the variables are integrated of order one, $I(1)$.
```{r ADF on the first difference}
diff_variables <- diff(variables)

adf_diff <- as.data.frame(matrix(nrow=ncol(diff_variables), ncol=4, NA))
colnames(adf_diff) <- c("Variable", "Test statistic", "Critical value", "Stationarity")
adf_diff$Variable <- colnames(diff_variables)

for (i in 1: ncol(diff_variables)){
  #lag_order            <- trunc((length(diff_variables[, i]) - 1)^(1/3))
  adf_test1             <- ur.df(diff_variables[, i], type="drift", lags=trunc((length(variables[, i]) - 1)^(1/3)), selectlags="AIC")
  adf_test2             <- ur.df(diff_variables[, i], type="none", lags=trunc((length(variables[, i]) - 1)^(1/3)), selectlags="AIC")
  if (colnames(diff_variables)[i] %in% c("GDP", "Consumer price index", "Stock prices")){
    test_statistic      <- adf_test1@teststat["statistic", "tau2"]
    critical_value_1pct <- adf_test1@cval["tau2", "1pct"]
  } else {
    test_statistic      <- adf_test2@teststat["statistic", "tau1"]
    critical_value_1pct <- adf_test2@cval["tau1", "1pct"]
  }
  adf_diff[i, "Test statistic"] <- round(as.numeric(test_statistic), 3)
  adf_diff[i, "Critical value"] <- round(as.numeric(critical_value_1pct), 3)
  adf_diff[i, "Stationarity"]   <- ifelse(test_statistic < critical_value_1pct, "Yes", "No")
  #adf_diff[i, "Lag order"]     <- lag_order
}

knitr::kable(adf_diff)
```
<div id="table4" style="text-align: center; color: #696969;">
###### Table 4: Augmented Dickey-Fuller test results on the first difference
</div>

# 3. Methodology

## 3.1 Model Specification

This study uses a Bayesian Structural vector autoregression (BSVAR) model to measure the dynamic and contemporaneous relationships between variables. The endogenous variables in the model are the following: 
$$
Y_t= 
\begin{pmatrix}
   gdp_t
 \\ICR_t
 \\cpi_t
 \\EXR_t
 \\stp_t
 \\vol_t
\end{pmatrix}
$$ 
$Y_t$ contains six variables ordered as

|    (1) Real GDP, $gdp_t$,
|    (2) Interest rates, $ICR_t$,
|    (3) Consumer price index, $cpi_t$,
|    (4) Exchange rates from AUD to USD, $EXR_t$,
|    (5) Stock prices, $stp_t$, and
|    (6) Stock price volatility, $vol_t$.

### Structural Form

The Structural VAR model can be represented as follows: 
$$
\begin{gather}
B_0Y_t = b_0 + \sum_{i=1}^{p} B_iY_{t-i} + u_t \\
u_t|Y_{t-1} \sim iid(0_N, I_N)
\end{gather}
$$ 
where

|    $Y_t$ is an $N \times 1$ vector of endogenous variables at time $t$,
|    $B_0$ is an $N \times N$ matrix capturing contemporaneous relationships between variables,
|    $u_t$ is an $N \times 1$ vector conditionally on $Y_{t-1}$ orthogonal structural shocks,
|    $N$ is the number of endogeneous variables, and $p$ is the lag length.

### Reduced Form

Through the transformation, the corresponding Structural VAR model can be represented as the reduced form VAR model as follows: 
$$
\begin{gather}
Y_t = \mu_0 + \sum_{i=1}^{p} A_iY_{t-i} + \epsilon_t \\
\epsilon_t|Y_{t-1} \sim iid(0_N, \Sigma)
\end{gather}
$$ 
where

|    $Y_t$ is an $N \times 1$ vector of endogenous variables at time $t$,
|    $A_i$ is an $N \times N$ matrix of autoregressive slope parameters,
|    $\mu_0$ is an $N \times 1$ vector of constant terms,
|    $\epsilon_t$ is an $N \times 1$ vector of the white noise error terms,
|    $\Sigma$ is an $N \times N$ covariance matrix of error terms $\epsilon_t$, where $\Sigma = B_0^{-1} {B_0^{-1}}'$,
|    $N$ is the number of endogeneous variables, and $p$ is the lag length.

## 3.2 Bayes' theorem

For parameter estimation, the **Bayes' theorem** is used to derive the joint posterior distribution.

The joint posterior distribution of $A$ and $\Sigma$ can be estimated as follows:
$$
\begin{align}
\underbrace{p(A,\Sigma|Y,X)}_{\text{Posterior}} &\propto L(A,\Sigma|Y,X) \cdot p(A,\Sigma) 
\\ &\propto \underbrace{L(A,\Sigma|Y,X)}_{\text{Likelihood function}} \cdot \underbrace{p(A|\Sigma) \cdot p(\Sigma)}_{\text{Prior}}
\end{align}
$$ 
## 3.3 Minnesota Prior

Using **Minnesota prior**, we set the prior mean of $\underline{A}$ and $\underline{V}$ as the following:

$$
\begin{align}
\underline{A} = \begin{bmatrix} \mathbf{0}_{N \times 1} \\ I_N \\ \mathbf{0}_{N \times (p-1)N} \end{bmatrix}
\end{align}
$$

$$
\begin{align}
\underline{A} &= \left[ \mathbf{0}_{N \times 1} \quad I_N \quad \mathbf{0}_{N \times (p-1)N} \right]'
\\ Var[vec(A)] &= \Sigma \otimes  \underline{V} 
\\ \underline{V} &= \text{diag}([\kappa_2 \quad \kappa_1 (\mathbf{p}^{-2} \otimes \imath'_N)]) 
\\ \mathbf{p} &= [1,2, ..., p], \qquad \imath_N = [1,...,1]
\end{align}
$$

|    $\kappa_1$ is overall shrinkage level of autoregressive slopes, where common value is $0.02^2$
|    $\kappa_2$ is overall shrinkage of the constant term



# 4. Estimation Framework

## 4.1 Standard Bayesian SVAR Model

### 4.1.1 Model Specification

The reduced form can be represented in a matrix form as follows: 
$$
\begin{gather}
Y = XA + E \\
\\ E|X \sim MN_{T \times N}(0_{T \times N},\Sigma_{N \times N},I_T) \\ 
\end{gather} 
$$ 
$$$$

$$
\begin{aligned}
Y = \begin{bmatrix} y_{1}' \\y_{2}'  \\. \\. \\. \\y_{T}' \end{bmatrix}_{T \times N} \quad
A = \begin{bmatrix}\mu_{0}' \\A_{1}' \\.\\.\\.\\A_{p}' \end{bmatrix}_{K \times N} \quad
x_t =\begin{bmatrix}\ 1 \\y_{t-1} \\.\\.\\.\\y_{t-p} \end{bmatrix}_{K \times 1} \quad
X = \begin{bmatrix}\ x_{1}' \\x_{2}' \\.\\.\\.\\x_{T}' \end{bmatrix}_{T \times K} \quad
E = \begin{bmatrix}\ \epsilon _{1}'  \\\epsilon _{2}' \\.\\.\\.\\\epsilon _{T}' \end{bmatrix}_{T \times N}
\end{aligned}
$$

where

|    $Y$ is a $T \times N$ matrix of endogenous variables,
|    $A$ is a $K \times N$ matrix of autoregressive slope parameters,
|    $X$ is a $T \times N$ matrix of covariates,
|    $E$ is a $T \times N$ matrix of the white noise error terms,
|    $\Sigma$ is an $N \times N$ row-specific covariance matrix of error terms 
|    $I_T$ is an $T \times T$ identity matrix representing the column-specific covariance matrix of error, and
|    $N$ is the number of endogeneous variables, 
|    $T$ is the number of time periods,
|    $p$ is the lag length, and
|    $K = 1 + pN$.

### 4.1.2 Estimation Procedure

For estimation, the **Bayes' theorem** is used to derive the joint posterior distribution for $A$ and $\Sigma$.
$$
\begin{align}
\underbrace{p(A,\Sigma|Y,X)}_{\text{Posterior}} &\propto L(A,\Sigma|Y,X) \cdot p(A,\Sigma) 
\\ &\propto \underbrace{L(A,\Sigma|Y,X)}_{\text{Likelihood function}} \cdot \underbrace{p(A|\Sigma) \cdot p(\Sigma)}_{\text{Prior}}
\end{align}
$$ 
This implies the following form for the kernel of the **likelihood function**: 
$$
\begin{align}
L(A,\Sigma|Y,X) &\propto \det(\Sigma)^{-\frac{T}{2}} \cdot \exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(Y-XA)'(Y-XA) \right] \right\} 
\\ &\propto \det(\Sigma)^{-\frac{T}{2}} 
\\ &\times \exp \left\{-\frac{1}{2} tr \left[\Sigma^{-1}(A-\hat{A})'X'X(A-\hat{A}) \right] \right\} 
\\ &\times \exp \left\{-\frac{1}{2} tr \left[\Sigma^{-1}(Y-X \hat{A})'(Y-X \hat{A}) \right] \right\}
\end{align}
$$
where 
$$
\begin{align}
\hat{A} &= (X'X)^{-1}X'Y
\\ \hat{\Sigma} &= \frac{1}{T} (Y-X \hat{A})'(Y-X \hat{A})
\end{align}
$$ 
are from the maximum likelihood estimation.

The **natural-conjugate prior distribution** where $A$ is matrix normal and $\Sigma$ follows inverse Wishart distribution has the same form as the joint posterior distribution for $A$ and $\Sigma$.
$$
\begin{gather}
p(A,\Sigma) = p(A|\Sigma) \cdot p(\Sigma) \\
\\ A|\Sigma \sim MN_{K \times N}(\underline{A},\Sigma,\underline{V}) \\
\\ \Sigma \sim IW_N(\underline{S},\underline{\nu})
\end{gather}
$$
This implies the following form for the kernel of the natural-conjugate prior distribution: 
$$
\begin{align}
p(A,\Sigma) &= p(A|\Sigma) \cdot p(\Sigma) \\
\\ &\propto \det(\Sigma)^{-\frac{N+K+\underline{v}+1}{2}} 
\\ &\times \exp \left\{-\frac{1}{2}tr \left[\Sigma^{-1}(A-\underline{A})'\underline{V}^{-1}(A-\underline{A}) \right] \right\} 
\\ &\times \exp \left\{-\frac{1}{2}tr \left[\Sigma^{-1}\underline{S} \right] \right\}
\end{align}
$$

The **posterior distribution** is given by the product of the likelihood and the priors.
$$
\begin{align}
p(A,\Sigma|Y,X) &\propto L(A,\Sigma|Y,X) \cdot p(A,\Sigma) \\
\\ &\propto L(A,\Sigma|Y,X) \cdot p(A|\Sigma) \cdot p(\Sigma) \\
\\ &\propto \det(\Sigma)^{-\frac{T}{2}} \cdot \exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\hat{A})'X'X(A-\hat{A}) \right] \right\} \cdot \exp \left\{-\frac{1}{2} tr \left[\Sigma^{-1}(Y-X \hat{A})'(Y-X \hat{A}) \right] \right\} 
\\ &\times \det(\Sigma)^{-\frac{N+K+\underline{v}+1}{2}} \cdot \exp\left\{-\frac{1}{2}tr[\Sigma^{-1}(A-\underline{A})'\underline{V}^{-1}(A-\underline{A})] \right\} \cdot \exp\left\{-\frac{1}{2}tr[\Sigma^{-1}\underline{S}] \right\} \\
\\ &\propto \det{(\Sigma)}^{-\frac{T+N+K+\underline{\nu}+1}{2}} 
\\ &\times \exp \left\{-\frac{1}{2} tr \left[\Sigma^{-1} \left[(A-\hat{A})'X'X(A-\hat{A})+(A-\underline{A})'\underline{V}^{-1}(A-\underline{A})+(Y-X\hat{A})'(Y-X\hat{A})+\underline{S} \right] \right] \right\} \\
\\ &\propto \det{(\Sigma)}^{-\frac{T+N+K+\underline{\nu}+1}{2}} 
\\ &\times \exp \left\{ -\frac{1}{2} tr \left[\Sigma^{-1}\left[(A-\overline{A})'\overline{V}^{-1} (A-\overline{A})+\underline{S}+Y'Y+\underline{A}'\underline{V}^{-1}\underline{A}-\overline{A}'\overline{V}^{-1} \overline{A} \right] \right] \right\}
\end{align}
$$ 

Combining the terms and completing the squares for the terms within the square brackets yields the following the joint posterior distributions for $A$ and $\Sigma$:
$$
\begin{gather}
p(A,\Sigma|Y,X) = p(A|Y,X,\Sigma) \cdot p(\Sigma|Y,X) = MNIW_{K \times N}(\overline{A}, \overline{V}, \overline{S}, \overline{\nu})
\\
\\ p(A|Y,X,\Sigma) = MN_{K \times N}(\overline{A}, \Sigma, \overline{V}) \\
\\ p(\Sigma|Y,X) = IW_N(\overline{S},\overline{\nu}) \\
\end{gather}
$$ 

where the parameter of the joint posterior distribution are the following: 
$$
\begin{align}
\overline{V} &= (X'X + \underline{V}^{-1})^{-1} 
\\ \overline{A} &= \overline{V}(X'Y + \underline{V}^{-1}\underline{A}) 
\\ \overline{\nu} &= T + \underline{\nu} 
\\ \overline{S} &= \underline{S} + Y'Y + \underline{A}'\underline{V}^{-1}\underline{A} - \overline{A}'\overline{V}^{-1}\overline{A}
\end{align}
$$

### 4.1.3 Algorithm Validation

To check the validity of the algorithms, two independent bi-variate Gaussian random walk processes with 1,000 observations were generated to simulate unit-root non-stationary macroeconomic variables.

```{r random walks}
set.seed(2024)
RW1 <- arima.sim(model=list(order=c(0, 1, 0)), n=1000, mean=0, sd=1)
RW2 <- arima.sim(model=list(order=c(0, 1, 0)), n=1000, mean=0, sd=1)
RW  <- cbind(RW1, RW2)
```

```{r random walks plot, fig.width=5, fig.height=3.5}
ggplot2::ggplot() +
  geom_line(aes(x=time(RW1), y=RW1), size=1, color="orange", linewidth=0.5) +
  geom_line(aes(x=time(RW1), y=RW2), size=1, color="skyblue", linewidth=0.5) +
  labs(title="Bi-variate Gaussian Random Walk Process", x="", y="") +
  theme_classic() +
  theme(legend.position="none", plot.title=element_text(hjust=0.5))
```

```{r, random walk baseline model posterior distribution, echo=TRUE}
#| code-fold: true
#| code-summary: "See R code"

RW.baseline <- function(data, sign.restrictions, p, S){
  
  # Define the lag order and the number of iterations
  ############################################################
  # p                       # number of lags
  # S                       # number of posterior draws
  
  # Create Y and X matrices
  ############################################################
  Y           <- data[(p+1):nrow(data), ]
  X           <- matrix(1, nrow(Y), 1)
  for (i in 1:p){
    X         <- cbind(X, data[(p+1):nrow(data)-i, ])
    }

  T           <- nrow(Y)    # number of time periods
  N           <- ncol(Y)    # number of variables
  K           <- 1+p*N
  
  # Convert it into a matrix form
  Y           <- as.matrix(Y)
  X           <- as.matrix(X)
  
  # Calculate the MLE
  ############################################################
  A.hat       <- solve(t(X)%*%X)%*%t(X)%*%Y                
  Sigma.hat   <- t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)  
  
  # Specify the prior distribution parameters
  ############################################################
  kappa.1     <- 0.02^2
  kappa.2     <- 100
  
  A.prior     <- matrix(0, nrow(A.hat), ncol(A.hat))
  A.prior[2:(N+1),] <- diag(N)
  V.prior     <- diag(c(kappa.2, kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior     <- diag(diag(Sigma.hat))
  nu.prior    <- N+1
  
  # Specify the matrix normal-inverse Wishart posterior parameters
  ############################################################
  V.bar.inv   <- t(X)%*%X + diag(1/diag(V.prior))
  V.bar       <- solve(V.bar.inv)
  A.bar       <- V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
  nu.bar      <- nrow(Y) + nu.prior
  S.bar       <- S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
  S.bar.inv   <- solve(S.bar)
  
  # Draw Posterior distribution
  ############################################################
  ## Draw from the Reduced Form
  ### Draw Sigma from the inverse Wishart distribution
  Sigma.posterior   <- rWishart(S, df=nu.bar, Sigma=S.bar.inv)
  Sigma.posterior   <- apply(Sigma.posterior, 3, solve)            
  Sigma.posterior   <- array(Sigma.posterior, c(N,N,S))
  
  ### Draw A from matrix-variate normal distribution
  A.posterior       <- array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S)) 
  
  # Initialise arrays to store posterior draws
  B0.posterior      <- array(NA, c(N,N,S))
  B1.posterior      <- array(NA, c(N,K,S))
  
  for (s in 1:S){
    ## Draw from the Structural Form
    ### Draw B0
    cholSigma.s        <- chol(Sigma.posterior[,,s])
    L                  <- t(chol(V.bar))
    B0.posterior[,,s]  <- solve(t(cholSigma.s)) 
    A.posterior[,,s]   <- A.bar + L%*%A.posterior[,,s]%*%cholSigma.s
    
    ### Draw Bplus
    B1.posterior[,,s]  <- B0.posterior[,,s]%*%t(A.posterior[,,s])
    }
  
  # Identification via sign restrictions
  ############################################################
  # Generate corresponding R matrix
  R <- diag(sign.restrictions)
  
  # Initialise arrays to store Q identified estimates
  i.vec <- c()
  Q.store      <- array(NA, c(N,N,S))
  B0.store     <- array(NA, c(N,N,S))
  B1.store     <- array(NA, c(N,K,S))
  
  for (s in 1:S){
    B0.tilde      <- B0.posterior[,,s]
    B1.tilde      <- B1.posterior[,,s]
    
    sign.restrictions.do.not.hold = TRUE
    i=1
    while (sign.restrictions.do.not.hold){
      X           <- matrix(rnorm(N*N), N, N)         
      QR          <- qr(X, tol=1e-10)
      Q           <- qr.Q(QR, complete=TRUE)
      R           <- qr.R(QR, complete=TRUE)
      Q           <- t(Q%*%diag(sign(diag(R))))
      B0          <- Q%*%B0.tilde                    
      B1          <- Q%*%B1.tilde                   
      B0.inv      <- solve(B0)      
      check       <- all(B0[1,1]>0, B0[2,2]>0)
      
      if (check){sign.restrictions.do.not.hold = FALSE}
      i=i+1
      }
    
    i.vec <- c(i.vec, i) 
    Q.store[,,s]   <- Q
    B0.store[,,s]  <- B0
    B1.store[,,s]  <- B1
    }
  
  B0.mean     <- apply(B0.store, 1:2, mean)  
  B1.mean     <- apply(B1.store, 1:2, mean)
  
  return(list(B0.mean = B0.mean, 
              B1.mean = B1.mean, 
              A.posterior = A.posterior, 
              Sigma.posterior = Sigma.posterior))
  }
```

```{r random walk baseline model result}
RW.baseline <- RW.baseline(data=RW, sign.restrictions=c(1, 1), p=1, S=1000)
```

The results show that 

```{r random walk baseline model Matrix B0}
#| label: table - RW Baseline B0
#| tbl-cap: Mean of the Matrix B[0]
knitr::kable(round(apply(RW.baseline$B0.mean, 1:2, mean), 4))
```

```{r random walk baseline model Matrix B+}
#| label: table - RW Baseline B+
#| tbl-cap: Mean of the Matrix B[+]
knitr::kable(round(apply(RW.baseline$B1.mean, 1:2, mean), 4))
```

```{r random walk baseline model A posterior}
#| label: table - RW Baseline A posterior
#| tbl-cap: Mean of the A posterior
knitr::kable(round(apply(RW.baseline$A.posterior, 1:2, mean), 4))
```

```{r random walk baseline model Sigma posterior}
#| label: table - RW Baseline Sigma posterior
#| tbl-cap: Mean of the Sigma posterior
knitr::kable(round(apply(RW.baseline$Sigma.posterior, 1:2, mean), 4))
```

## 4.2 Bayesian SVAR Model with *t*-distributed Innovations

### 4.2.1 Model Specification

Stock price volatility

$$
\begin{align}
E_T &\sim t_N(0, \Sigma, \nu) 
\end{align}
$$
Then, the reduced form can be represented in a matrix form as follows:
$$
\begin{gather}
Y = XA + E \\
\\ E|X, \lambda \sim MN_{T \times N}(0_{T \times N},\Sigma_{N \times N},\lambda I_T)
\end{gather} 
$$ 
where lambda is inverse gamma 2 distributed with shape and ..

$$
\lambda \sim IG2(s_{\lambda}, \nu_{\lambda})
$$




### 4.2.2 Estimation Procedure

Using the **Bayes' theorem**, the joint posterior distribution for $A$ and $\Sigma$ is the following:
$$
\begin{align}
\underbrace{p(A,\Sigma|Y,X)}_{\text{Posterior}} &\propto L(A,\Sigma|Y,X) \cdot p(A,\Sigma) 
\\ &\propto \underbrace{L(A,\Sigma|Y,X)}_{\text{Likelihood function}} \cdot \underbrace{p(A|\Sigma) \cdot p(\Sigma)}_{\text{Prior}}
\end{align}
$$ 

The kernel of the **likelihood function** follows as:
$$
\begin{align}
L(A,\Sigma|Y,X,\lambda) &\propto \det(\Sigma)^{-\frac{T}{2}} \cdot \det(\lambda I_T)^{-\frac{N}{2}} \cdot \exp \left\{-\frac{1}{2} \text{tr} \left[\Sigma^{-1}(Y-XA)'(\lambda I_T)^{-1}(Y-XA) \right] \right\}
\end{align}
$$
The **conditional posterior distribution of** $A$ **and** $\Sigma$:
$$
\begin{align}
p(A,\Sigma|Y,X,\lambda) &\propto L(A,\Sigma|Y,X,\lambda) \cdot p(A,\Sigma) \\
\\ &\propto L(A,\Sigma|Y,X,\lambda) \cdot p(A|\Sigma,\lambda) \cdot p(\Sigma|\lambda) \\
\\ &\propto \det(\Sigma)^{-\frac{T}{2}} \cdot \det(\lambda I_T)^{-\frac{N}{2}} \cdot \exp \left\{-\frac{1}{2} tr[\Sigma^{-1} (Y-XA)' (\lambda I_T)^{-1} (Y-XA) ] \right\}
\\ &\times 
\det(\Sigma)^{-\frac{N+k+\underline{\nu}+1}{2}} \cdot \exp \left\{-\frac{1}{2}tr[\Sigma^{-1}(A-\underline{A})'\underline{V}^{-1}(A-\underline{A})] \right\} \cdot \exp \left\{ -\frac{1}{2}tr[\Sigma^{-1}\underline{S}] \right\} \\
\\ &\propto \det(\Sigma)^{-\frac{T+N+K+\underline{\nu}+1}{2}} \cdot \det(\lambda I_T)^{-\frac{N}{2}} 
\\ &\times \exp \left\{-\frac{1}{2} tr[\Sigma^{-1}(Y'(\lambda I_T)^{-1}Y - 2A'X'(\lambda I_T)^{-1}Y + A'X'(\lambda I_T)^{-1}XA + A'\underline{V}^{-1}A -2A'\underline{V}^{-1}\underline{A} + \underline{A}'\underline{V}^{-1}\underline{A} + \underline{S})] \right\}
\end{align}
$$
This follows:
$$
\begin{gather}
p(A,\Sigma|Y,X,\lambda) = p(A|Y,X,\Sigma, \lambda) \cdot p(\Sigma|Y,X,\lambda) = MNIW_{K \times N}(\overline{A}, \overline{V}, \overline{S}, \overline{\nu}) \\
\\ p(A|Y,X,\Sigma,\lambda) = MN_{K \times N}(\overline{A}, \Sigma, \overline{V}) \\
\\ p(\Sigma|Y,X,\lambda) = IW_N(\overline{S},\overline{\nu}) \\
\end{gather}
$$
where
$$
\begin{align}
\overline{V} &= [X'(\lambda I_T)^{-1}X + \underline{V}^{-1}]^{-1} \\
\overline{A} &= \overline{V}[X'(\lambda I_T)^{-1}Y + \underline{V}^{-1}\underline{A}] \\
\overline{S} &= Y'(\lambda I_T)^{-1}Y + \underline{A}'\underline{V}^{-1}\underline{A} - \overline{A}'\overline{V}^{-1}\overline{A} + \underline{S} \\
\overline{\nu} &= \underline{\nu} + T
\end{align}
$$

The **conditional posterior distribution of** $\lambda$:
$$
\begin{align}
p(\lambda|Y,X,A,\Sigma) &\propto L(A,\Sigma,\lambda|Y,X) \cdot p(A,\Sigma) \cdot p(\lambda) \\
\\ &\propto L(A,\Sigma,\lambda|Y,X) \cdot p(\lambda) \\
\\ &\propto \det(\Sigma)^{-\frac{T}{2}} \cdot \det(\lambda I_T)^{-\frac{N}{2}} \cdot
\exp \left\{-\frac{1}{2} tr[\Sigma^{-1} (Y-XA)' (\lambda I_T)^{-1} (Y-XA)] \right\}
\\ &\times \lambda^{-\frac{\underline{\nu_{\lambda}}+2}{2}} \cdot 
\exp \left\{-\frac{1}{2}\frac{\underline{s_{\lambda}}}{\lambda} \right\} \\
\\ &\propto \det(\Sigma)^{-\frac{T}{2}} \cdot \det(I_T)^{-\frac{N}{2}} \cdot 
\exp \left\{-\frac{1}{2}\frac{1}{\lambda} tr[\Sigma^{-1}(Y-XA)'(Y-XA)] \right\} \\
&\times \lambda^{-\frac{TN}{2}} \cdot \lambda^{-\frac{\underline{\nu_{\lambda}}+2}{2}} \cdot 
\exp \left\{-\frac{1}{2}\frac{\underline{s_{\lambda}}}{\lambda} \right\} \\
\\ &\propto \det(\Sigma)^{-\frac{T}{2}} \cdot \det(I_T)^{-\frac{N}{2}} \cdot \lambda^{-\frac{TN+\underline{\nu_{\lambda}}+2}{2}} \cdot \exp \left\{-\frac{1}{2}\frac{1}{\lambda} [tr(\Sigma^{-1}(Y-XA)'(Y-XA)) + \underline{s_{\lambda}}] \right\}
\end{align}
$$
This follows
$$
\begin{align}
\lambda|Y,X, A,\Sigma &\sim IG2(\overline{s_{\lambda}},\overline{\nu_{\lambda}})
\end{align}
$$
where
$$
\begin{align}
\overline{s_{\lambda}} &= tr[\Sigma^{-1}(Y-XA)'(Y-XA)] + \underline{s_{\lambda}} \\
\overline{\nu_{\lambda}} &= \underline{\nu_{\lambda}} + TN 
\end{align}
$$

### 4.2.3 Algorithm Validation

To check the validity of the algorithms, two independent bi-variate Gaussian random walk processes with 1,000 observations were generated to simulate unit-root non-stationary macroeconomic variables.

#### Gibbs Sampler

Initialise $\lambda$ at $\lambda^{(0)}$
From S = 1, 

1. Draw $\Sigma^{(s)} \sim P(\Sigma|Y,X,\lambda^{(s-1)})$ from the $IW_N(\overline{S},\overline{\nu})$ distribution
2. Draw $A^{(s)} \sim P(A|Y,X,\Sigma^{(s)},\lambda^{(s-1)})$ from the $MN_{K \times N}(\overline{A},\Sigma^{(s)}, \overline{V})$ distribution
3. Draw $\lambda^{(s)} \sim P(\lambda|Y,X,A^{(s)},\Sigma^{(s)})$ from $IG2(\overline{s_{\lambda}},\overline{\nu_{\lambda}})$ distribution

```{r random walks 2}
set.seed(2024)
RW11 <- arima.sim(model=list(order=c(0, 1, 0)), n=1000, mean=0, sd=1)
RW22 <- arima.sim(model=list(order=c(0, 1, 0)), n=1000, mean=0, sd=1)
RW12  <- cbind(RW11, RW22)
```

```{r, random walk extended model posterior distribution, echo=TRUE}
#| code-fold: true
#| code-summary: "See R code"

RW.extended <- function(data, sign.restrictions, p, S){
   
  # Define the lag order and the number of iterations
  ############################################################
  # p                       # number of lags
  # S                       # number of posterior draws

  # Create Y and X matrices
  ############################################################
  Y           <- data[(p+1):nrow(data), ]
  X           <- matrix(1, nrow(Y), 1)
  for (i in 1:p){
    X         <- cbind(X, data[(p+1):nrow(data)-i, ])
    }
  
  T           <- nrow(Y)    # number of time periods
  N           <- ncol(Y)    # number of variables
  K           <- 1+p*N
  
  # Convert it into a matrix form
  Y           <- as.matrix(Y)
  X           <- as.matrix(X)
  
  # Calculate the MLE
  ############################################################
  A.hat       <- solve(t(X)%*%X)%*%t(X)%*%Y                
  Sigma.hat   <- t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  
  # Specify prior distribution parameters
  ############################################################
  kappa.1     <- 0.02^2
  kappa.2     <- 100
  
  A.prior           <- matrix(0, nrow(A.hat), ncol(A.hat))
  A.prior[2:(N+1),] <- diag(N)
  V.prior           <- diag(c(kappa.2, kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior           <- diag(diag(Sigma.hat))
  nu.prior          <- N+1
  s.prior.lambda    <- 5   # assume that it is fixed
  nu.prior.lambda   <- 5   # assume that it is fixed
  lambda            <- s.prior.lambda/rchisq(1, nu.prior.lambda)
  
  # Initialise arrays to store posterior draws
  Sigma.posterior   <- array(NA, c(N,N,S))
  A.posterior       <- array(NA, c(K,N,S))
  lambda.posterior  <- rep(NA, S)
  B0.posterior      <- array(NA, c(N,N,S))
  B1.posterior      <- array(NA, c(N,K,S))
  
  for (s in 1:S){
    
    # Specify the matrix normal-inverse Wishart posterior parameters
    ############################################################
    V.bar.inv  <- t(X)%*%X/lambda + diag(1/diag(V.prior))
    V.bar      <- solve(V.bar.inv)
    A.bar      <- V.bar%*%(t(X)%*%Y/lambda + diag(1/diag(V.prior))%*%A.prior)
    nu.bar     <- nrow(Y) + nu.prior
    S.bar      <- S.prior + t(Y)%*%Y/lambda + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv  <- solve(S.bar)
    
    # Draw Posterior distribution
    ############################################################
    ## Draw from the Reduced Form
    ### Draw Sigma from the inverse Wishart distribution
    Sigma.posterior.inv   <- rWishart(1, df=nu.bar, Sigma=S.bar.inv)[,,1]
    Sigma.posterior[,,s]  <- solve(Sigma.posterior.inv)
    
    ### Draw A from matrix-variate normal distribution
    A.posterior[,,s]      <- matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.bar), sigma=Sigma.posterior[,,s]%x%V.bar), ncol=N)
    
    ### Draw lambda from inverse gamma 2 distribution
    s.posterior.lambda    <- sum(diag(Sigma.posterior.inv%*%t(Y-X%*%A.posterior[,,s])%*%(Y-X%*%A.posterior[,,s]))) + s.prior.lambda
    nu.posterior.lambda   <- nrow(Y)*N + nu.prior.lambda
    lambda                <- s.posterior.lambda/rchisq(1, nu.posterior.lambda)
    lambda.posterior[s]   <- lambda
  
    ## Draw from the Structural Form
    ### Draw B0
    cholSigma.s           <- chol(Sigma.posterior[,,s])
    B0.posterior[,,s]     <- solve(t(cholSigma.s)) 
    
    ### Draw Bplus
    B1.posterior[,,s]     <- B0.posterior[,,s]%*%t(A.posterior[,,s])
    }
  
  # Identification via sign restrictions 
  ############################################################
  # Generate corresponding R matrix
  R <- diag(sign.restrictions)
  
  # Initialise arrays to store Q identified estimates
  i.vec <- c()
  Q.store      <- array(NA, c(N,N,S))
  B0.store     <- array(NA, c(N,N,S))
  B1.store     <- array(NA, c(N,K,S))
  
  for (s in 1:S){
    B0.tilde      <- B0.posterior[,,s]
    B1.tilde      <- B1.posterior[,,s]
    
    sign.restrictions.do.not.hold = TRUE
    i=1
    while (sign.restrictions.do.not.hold){
      X           <- matrix(rnorm(N*N), N, N)         
      QR          <- qr(X, tol=1e-10)
      Q           <- qr.Q(QR, complete=TRUE)
      R           <- qr.R(QR, complete=TRUE)
      Q           <- t(Q%*%diag(sign(diag(R))))
      B0          <- Q%*%B0.tilde                    
      B1          <- Q%*%B1.tilde                   
      B0.inv      <- solve(B0)      
      check       <- all(B0[1,1]>0, B0[2,2]>0)
      
      if (check){sign.restrictions.do.not.hold = FALSE}
      i=i+1
      }
    
    i.vec <- c(i.vec, i) 
    Q.store[,,s]   <- Q
    B0.store[,,s]  <- B0
    B1.store[,,s]  <- B1
    }
  
  B0.mean     <- apply(B0.store, 1:2, mean)  
  B1.mean     <- apply(B1.store, 1:2, mean)
  
  return(list(B0.mean = B0.mean, 
              B1.mean = B1.mean, 
              A.posterior = A.posterior, 
              Sigma.posterior = Sigma.posterior,
              lambda.posterior = lambda.posterior))
  }
```

```{r random walk extended model result}
RW.extended <- RW.extended(data=RW12, sign.restrictions=c(1, 1), p=1, S=1000)
```

The results shows that

```{r random walk extended model Matrix B0}
#| label: table - RW Extended B0
#| tbl-cap: Mean of the Matrix B[0]
knitr::kable(round(apply(RW.extended$B0.mean, 1:2, mean), 4))
```

```{r random walk extended model Matrix B+}
#| label: table - RW Extended B+
#| tbl-cap: Mean of the Matrix B[+]
knitr::kable(round(apply(RW.extended$B1.mean, 1:2, mean), 4))
```

```{r random walk extended model A posterior}
#| label: table - RW Extended A Posterior
#| tbl-cap: Mean of the A posterior
knitr::kable(round(apply(RW.extended$A.posterior, 1:2, mean), 4))
```

```{r random walk extended model Sigma posterior}
#| label: table - RW Extended Sigma Posterior
#| tbl-cap: Mean of the Sigma posterior
knitr::kable(round(apply(RW.extended$Sigma.posterior, 1:2, mean), 4))
```

```{r random walk extended model lambda}
#| label: table - RW Extended lambda
#| tbl-cap: Mean of the lambda
knitr::kable(round(mean(RW.extended$lambda.posterior), 4), col.names=c("lambda"))
```

## 4.3 Bayesian SVAR Model with Common Stochastic Volatility

### 4.3.1 Model Specification


### 4.3.2 Estimation Procedure


### 4.3.3 Algorithm Validation



# 5. Empirical Results

## 5.1 Standard Bayesian SVAR model


$$
f(B_0,B_+)=\Theta_0=B=
\begin{bmatrix}
* & - & * & * & * & * \\
* & + & * & * & * & * \\
* & - & * & * & * & * \\
* & + & * & * & * & * \\
* & * & * & * & * & * \\
* & * & * & * & * & * \\
\end{bmatrix}
$$

$$
\textbf{R}=\begin{bmatrix}
-1 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & -1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 \\
\end{bmatrix}
$$

1990 Q2 onwards..

```{r dataset}
# Convert the data type
variables <- data.frame(lapply(variables, as.numeric))
```

```{r baseline model posterior distribution}

baseline <- function(data, sign.restrictions, p, S){
  
  # Define the lag order and the number of iterations
  ############################################################
  # p                       # number of lags (data frequency (quarter))
  # S                       # number of posterior draws

  # Create Y and X matrices
  ############################################################
  Y           <- data[(p+1):nrow(data), ]
  X           <- matrix(1, nrow(Y), 1)
  for (i in 1:p){
    X         <- cbind(X, data[(p+1):nrow(data)-i, ])
    }
  
  T           <- nrow(Y)    # number of time periods
  N           <- ncol(Y)    # number of variables
  K           <- 1+p*N
  
  # Convert it into a matrix form
  Y           <- as.matrix(Y)
  X           <- as.matrix(X)
  
  # Calculate the MLE
  ############################################################
  A.hat       <- solve(t(X)%*%X)%*%t(X)%*%Y                
  Sigma.hat   <- t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)  
  
  # Specify the prior distribution parameters
  ############################################################
  kappa.1     <- 1
  kappa.2     <- 100
  
  A.prior     <- matrix(0, nrow(A.hat), ncol(A.hat))
  A.prior[2:(N+1),] <- diag(N)
  V.prior     <- diag(c(kappa.2, kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior     <- diag(diag(Sigma.hat))
  nu.prior    <- N+1
  
  # Specify the matrix normal-inverse Wishart posterior parameters
  ############################################################
  V.bar.inv   <- t(X)%*%X + diag(1/diag(V.prior))
  V.bar       <- solve(V.bar.inv)
  A.bar       <- V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
  nu.bar      <- nrow(Y) + nu.prior
  S.bar       <- S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
  S.bar.inv   <- solve(S.bar)
  
  # Draw Posterior distribution
  ############################################################
  ## Draw from the Reduced Form
  ### Draw Sigma from the inverse Wishart distribution
  Sigma.posterior   <- rWishart(S, df=nu.bar, Sigma=S.bar.inv)
  Sigma.posterior   <- apply(Sigma.posterior, 3, solve)            
  Sigma.posterior   <- array(Sigma.posterior, c(N,N,S))
  
  ### Draw A from matrix-variate normal distribution
  A.posterior       <- array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S)) 
  
  # Initialise arrays to store posterior draws
  B0.posterior      <- array(NA, c(N,N,S))
  B1.posterior      <- array(NA, c(N,K,S))
  
  for (s in 1:S){
    ## Draw from the Structural Form
    ### Draw B0
    cholSigma.s        <- chol(Sigma.posterior[,,s])
    L                  <- t(chol(V.bar))
    B0.posterior[,,s]  <- solve(t(cholSigma.s)) 
    A.posterior[,,s]   <- A.bar + L%*%A.posterior[,,s]%*%cholSigma.s
    
    ### Draw Bplus
    B1.posterior[,,s]  <- B0.posterior[,,s]%*%t(A.posterior[,,s])
    }
  
  # Identification via sign restrictions
  ############################################################
  # Generate corresponding R matrix
  R <- diag(sign.restrictions)
  
  # Initialise arrays to store Q identified estimates
  i.vec <- c()
  Q.store      <- array(NA, c(N,N,S))
  B0.store     <- array(NA, c(N,N,S))
  B1.store     <- array(NA, c(N,K,S))
  
  for (s in 1:S){
    B0.tilde      <- B0.posterior[,,s]
    B1.tilde      <- B1.posterior[,,s]
    
    sign.restrictions.do.not.hold = TRUE
    i=1
    while (sign.restrictions.do.not.hold){
      X           <- matrix(rnorm(N*N), N, N)         
      QR          <- qr(X, tol=1e-10)
      Q           <- qr.Q(QR, complete=TRUE)
      R           <- qr.R(QR, complete=TRUE)
      Q           <- t(Q%*%diag(sign(diag(R))))
      B0          <- Q%*%B0.tilde                    
      B1          <- Q%*%B1.tilde                   
      B0.inv      <- solve(B0)      
      check       <- all(B0.inv[1,2]<0, B0.inv[2,2]>0, B0.inv[3,2]<0, B0.inv[4,2]>0)
      
      if (check){sign.restrictions.do.not.hold = FALSE}
      i=i+1
      }
    
    i.vec <- c(i.vec, i) 
    Q.store[,,s]   <- Q
    B0.store[,,s]  <- B0
    B1.store[,,s]  <- B1
    }
  
  B0.mean     <- apply(B0.store, 1:2, mean)  
  B1.mean     <- apply(B1.store, 1:2, mean)

  return(list(B0.mean = B0.mean, 
              B1.mean = B1.mean, 
              A.posterior = A.posterior, 
              Sigma.posterior = Sigma.posterior))
  }
```

```{r baseline model result}
baseline <- baseline(data=variables, sign.restrictions=c(-1, 1, -1, 1, 0, 0), p=4, S=50000)
```

```{r baseline model Matrix B0}
#| label: table - Baseline Model B0
#| tbl-cap: Mean of the Matrix B[0]
knitr::kable(round(apply(baseline$B0.mean, 1:2, mean), 4))
```

```{r baseline model Matrix B+}
#| label: table - Baseline Model B+
#| tbl-cap: Mean of the Matrix B[+]
knitr::kable(round(apply(baseline$B1.mean, 1:2, mean), 4))
```

```{r define colours for impulse response function}
# Define colors
############################################################
mcxs1  <- "#05386B"
mcxs2  <- "#379683"
mcxs3  <- "#5CDB95"
mcxs4  <- "#8EE4AF"
mcxs5  <- "#EDF5E1"
purple <- "#b02442"
  
mcxs1.rgb    <- col2rgb(mcxs1)
mcxs1.shade1 <- rgb(mcxs1.rgb[1],mcxs1.rgb[2],mcxs1.rgb[3], alpha=120, maxColorValue=255)
mcxs2.rgb    <- col2rgb(mcxs2)
mcxs2.shade1 <- rgb(mcxs2.rgb[1],mcxs2.rgb[2],mcxs2.rgb[3], alpha=120, maxColorValue=255)
```

Impulse response functions of the baseline model show a positive monetary policy shock on variables. The shaded area represents 68% of the confidence interval. In the short run,  GDP decreases gradually, but a year after the shock, GDP decreases at a steeper rate. In the short run, interest rates immediately respond to the shock and normalised after five years of the shock. The consumer price index slightly decreases in the short run. The exchange rate has a positive effect in the short and long run. The stock price has a positive effect in the short run, but it does not have a positive effect in the long run.

```{r baseline model irf, echo=TRUE}
#| code-fold: true
#| code-summary: "See R code"

# Define the lag order and the number of iterations
############################################################
p           <- 4          # number of lags (data frequency (quarter))
S           <- 50000      # number of posterior draws
h           <- 20         # forecast horizon

# Create Y and X matrices
############################################################
Y           <- variables[(p+1):nrow(variables), ]
X           <- matrix(1, nrow(Y), 1)
for (i in 1:p){
  X         <- cbind(X, variables[(p+1):nrow(variables)-i, ])
  }

T           <- nrow(Y)    # number of time periods
N           <- ncol(Y)    # number of variables
K           <- 1+p*N

# Convert it into a matrix form
Y           <- as.matrix(Y)
X           <- as.matrix(X)

# Calculate the MLE
############################################################
A.hat       <- solve(t(X)%*%X)%*%t(X)%*%Y                
Sigma.hat   <- t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)  
  
# Specify the prior distribution parameters
############################################################
kappa.1     <- 1
kappa.2     <- 100
  
A.prior     <- matrix(0, nrow(A.hat), ncol(A.hat))
A.prior[2:(N+1),] <- diag(N)
V.prior     <- diag(c(kappa.2, kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior     <- diag(diag(Sigma.hat))
nu.prior    <- N+1
  
# Specify the matrix normal-inverse Wishart posterior parameters
############################################################
V.bar.inv   <- t(X)%*%X + diag(1/diag(V.prior))
V.bar       <- solve(V.bar.inv)
A.bar       <- V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
nu.bar      <- nrow(Y) + nu.prior
S.bar       <- S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
S.bar.inv   <- solve(S.bar)
  
# Draw Posterior distribution
############################################################
## Draw from the Reduced Form
### Draw Sigma from the inverse Wishart distribution
Sigma.posterior   <- rWishart(S, df=nu.bar, Sigma=S.bar.inv)
Sigma.posterior   <- apply(Sigma.posterior, 3, solve)            
Sigma.posterior   <- array(Sigma.posterior, c(N,N,S))

### Draw A from matrix-variate normal distribution
A.posterior       <- array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S)) 
  
# Initialise arrays to store posterior draws
B0.posterior      <- array(NA, c(N,N,S))
B1.posterior      <- array(NA, c(N,K,S))
  
for (s in 1:S){
  ## Draw from the Structural Form
  ### Draw B0
  cholSigma.s        <- chol(Sigma.posterior[,,s])
  L                  <- t(chol(V.bar))
  B0.posterior[,,s]  <- solve(t(cholSigma.s)) 
  A.posterior[,,s]   <- A.bar + L%*%A.posterior[,,s]%*%cholSigma.s
  
  ### Draw Bplus
  B1.posterior[,,s]  <- B0.posterior[,,s]%*%t(A.posterior[,,s])
  }
  
# Identification via sign restrictions
############################################################
# Generate corresponding R matrix
sign.restrictions <- c(-1, 1, -1, 1, 0, 0)
R <- diag(sign.restrictions)
  
# Initialise arrays to store Q identified estimates
i.vec <- c()
Q.store      <- array(NA, c(N,N,S))
B0.store     <- array(NA, c(N,N,S))
B1.store     <- array(NA, c(N,K,S))

for (s in 1:S){
  B0.tilde      <- B0.posterior[,,s]
  B1.tilde      <- B1.posterior[,,s]
    
  sign.restrictions.do.not.hold = TRUE
  i=1
  while (sign.restrictions.do.not.hold){
    X           <- matrix(rnorm(N*N), N, N)         
    QR          <- qr(X, tol=1e-10)
    Q           <- qr.Q(QR, complete=TRUE)
    R           <- qr.R(QR, complete=TRUE)
    Q           <- t(Q%*%diag(sign(diag(R))))
    B0          <- Q%*%B0.tilde                    
    B1          <- Q%*%B1.tilde                   
    B0.inv      <- solve(B0)      
    check       <- all(B0.inv[1,2]<0, B0.inv[2,2]>0, B0.inv[3,2]<0, B0.inv[4,2]>0)
    
    if (check){sign.restrictions.do.not.hold = FALSE}
    i=i+1
    }
  
  i.vec <- c(i.vec, i) 
  Q.store[,,s]   <- Q
  B0.store[,,s]  <- B0
  B1.store[,,s]  <- B1
  }

# Impulse response functions
############################################################
# Transform B0 matrices to B
# Initialise arrays to store posterior draws
B.posterior <- array(NA, c(N,N,S))
  
for (s in 1:S){
  B.posterior[,,s]  <- solve(B0.store[,,s])
  }

# Initialise arrays to store posterior draws
IRF.posterior       <- array(NA, c(N,N,h+1,S))
IRF.inf.posterior   <- array(NA, c(N,N,S))
J                   <- cbind(diag(N), matrix(0,N,N*(p-1)))
  
for (s in 1:S){
  # Define A matrix in VAR(1) representation
  A.bold                     <- rbind(t(A.posterior[2:(1+N*p),,s]), cbind(diag(N*(p-1)), matrix(0,N*(p-1),N)))
  IRF.inf.posterior[,,s]     <- J%*%solve(diag(N*p)-A.bold)%*%t(J)%*%B.posterior[,,s]
  A.bold.power               <- A.bold
  
  for (i in 1:(h+1)){
    if (i==1){
      IRF.posterior[,,i,s]      <- B.posterior[,,s]
      } else {
        IRF.posterior[,,i,s]    <- J%*%A.bold.power%*%t(J)%*%B.posterior[,,s]
        A.bold.power            <- A.bold.power%*%A.bold
      }
    }
  }

library(HDInterval)

# Impulse response functions plots
############################################################
IRF.posterior.mps   <- IRF.posterior[,2,,]
IRFs.k1             <- apply(IRF.posterior.mps,1:2,median)
IRFs.inf.k1         <- apply(IRF.posterior.mps,1,mean)
rownames(IRFs.k1)   <- colnames(variables)

IRFs.k1.hdi    <- apply(IRF.posterior.mps, 1:2, hdi, credMass=0.68)
hh             <- 1:(h+1)

par(mfrow=c(3, 2), mar=c(3, 3, 2, 2), cex.axis=1.5, cex.lab=1.5)
for (n in 1:N){
  ylims=range(IRFs.k1[n,hh], IRFs.k1.hdi[,n,1:(h+1)], 0)
  plot(hh,IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="", main=rownames(IRFs.k1)[n])
  if (n==5){
    axis(1, c(1,5,9,13,17,21), c("0","1yr","2yr","3yr","4yr","5yr"))
    } else {
      axis(1, c(1,5,9,13,17,21), c("0","1yr","2yr","3yr","4yr","5yr"))
      }
  axis(2, c(ylims[1], 0, ylims[2]), round(c(ylims[1], 0, ylims[2]), 3))
  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh], IRFs.k1.hdi[2,n,(h+1):1]), col=mcxs1.shade1, border=mcxs1.shade1)
  abline(h=0)
  lines(hh, IRFs.k1[n,hh], lwd=2, col="darkblue")
  }
```


## 5.2 Bayesian SVAR Model with *t*-distributed Innovations

```{r, extended model posterior distribution}

extended <- function(data, sign.restrictions, p, S) {
  
  # Define the lag order and the number of iterations
  ############################################################
  # p                       # number of lags (data frequency (quarter))
  # S                       # number of posterior draws

  # Create Y and X matrices
  ############################################################
  Y           <- data[(p+1):nrow(data), ]
  X           <- matrix(1, nrow(Y), 1)
  for (i in 1:p){
    X         <- cbind(X, data[(p+1):nrow(data)-i, ])
    }
  
  T           <- nrow(Y)    # number of time periods
  N           <- ncol(Y)    # number of variables
  K           <- 1+p*N
  
  # Convert it into a matrix form
  Y           <- as.matrix(Y)
  X           <- as.matrix(X)
  
  # Calculate the MLE
  ############################################################
  A.hat       <- solve(t(X)%*%X)%*%t(X)%*%Y                
  Sigma.hat   <- t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  
  # Specify prior distribution parameters
  ############################################################
  kappa.1     <- 1
  kappa.2     <- 100
  
  A.prior           <- matrix(0, nrow(A.hat), ncol(A.hat))
  A.prior[2:(N+1),] <- diag(N)
  V.prior           <- diag(c(kappa.2, kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior           <- diag(diag(Sigma.hat))
  nu.prior          <- N+1
  s.prior.lambda    <- 5     # assume that it is fixed
  nu.prior.lambda   <- 5     # assume that it is fixed
  lambda            <- s.prior.lambda/rchisq(1, nu.prior.lambda)
  
  # Initialise arrays to store posterior draws
  Sigma.posterior   <- array(NA, c(N,N,S))
  A.posterior       <- array(NA, c(K,N,S))
  lambda.posterior  <- rep(NA, S)
  B0.posterior      <- array(NA, c(N,N,S))
  B1.posterior      <- array(NA, c(N,K,S))
  
  for (s in 1:S){
    
    # Specify the matrix normal-inverse Wishart posterior parameters
    ############################################################
    V.bar.inv   <- t(X)%*%X/lambda + diag(1/diag(V.prior))
    V.bar       <- solve(V.bar.inv)
    A.bar       <- V.bar%*%(t(X)%*%Y/lambda + diag(1/diag(V.prior))%*%A.prior)
    nu.bar      <- nrow(Y) + nu.prior
    S.bar       <- S.prior + t(Y)%*%Y/lambda + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv   <- solve(S.bar)
    
    # Draw Posterior distribution
    ############################################################
    ## Draw from the Reduced Form
    ### Draw Sigma from the inverse Wishart distribution
    Sigma.posterior.inv   <- rWishart(1, df=nu.bar, Sigma=S.bar.inv)[,,1]
    Sigma.posterior[,,s]  <- solve(Sigma.posterior.inv)
    
    ### Draw A from matrix-variate normal distribution
    A.posterior[,,s]      <- matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.bar), sigma=Sigma.posterior[,,s]%x%V.bar), ncol=N)
    
    ### Draw lambda from inverse gamma 2 distribution
    s.posterior.lambda    <- sum(diag(Sigma.posterior.inv%*%t(Y-X%*%A.posterior[,,s])%*%(Y-X%*%A.posterior[,,s]))) + s.prior.lambda
    nu.posterior.lambda   <- nrow(Y)*N + nu.prior.lambda
    lambda   <- s.posterior.lambda/rchisq(1, nu.posterior.lambda)
    lambda.posterior[s]   <- lambda
    
    ## Draw from the Structural Form
    ### Draw B0
    cholSigma.s        <- chol(Sigma.posterior[,,s])
    B0.posterior[,,s]  <- solve(t(cholSigma.s))
    
    ### Draw Bplus
    B1.posterior[,,s]  <- B0.posterior[,,s]%*%t(A.posterior[,,s])
    }
  
  # Identification via sign restrictions 
  ############################################################
  # Generate corresponding R matrix
  R <- diag(sign.restrictions)
  
  # Initialise arrays to store Q identified estimates
  i.vec <- c()
  Q.store      <- array(NA, c(N,N,S))
  B0.store     <- array(NA, c(N,N,S))
  B1.store     <- array(NA, c(N,K,S))
  
  for (s in 1:S){
    B0.tilde      <- B0.posterior[,,s]
    B1.tilde      <- B1.posterior[,,s]
    
    sign.restrictions.do.not.hold = TRUE
    i=1
    while (sign.restrictions.do.not.hold){
      X           <- matrix(rnorm(N*N), N, N)         
      QR          <- qr(X, tol=1e-10)
      Q           <- qr.Q(QR, complete=TRUE)
      R           <- qr.R(QR, complete=TRUE)
      Q           <- t(Q %*% diag(sign(diag(R))))
      B0          <- Q%*%B0.tilde                    
      B1          <- Q%*%B1.tilde                   
      B0.inv      <- solve(B0)      
      check       <- all(B0[1,1]<0, B0[2,2]>0, B0[3,3]<0, B0[4,4]>0)
      
      if (check){sign.restrictions.do.not.hold = FALSE}
      i=i+1
      }
    
    i.vec <- c(i.vec, i) 
    Q.store[,,s]   <- Q
    B0.store[,,s]  <- B0
    B1.store[,,s]  <- B1
    }
  
  B0.mean     <- apply(B0.store, 1:2, mean)  
  B1.mean     <- apply(B1.store, 1:2, mean)
  
  return(list(B0.mean = B0.mean, 
              B1.mean = B1.mean, 
              A.posterior = A.posterior, 
              Sigma.posterior = Sigma.posterior,
              lambda.posterior = lambda.posterior))
  }
```

```{r extended model result}
extended <- extended(data=variables, sign.restrictions=c(-1, 1, -1, 1, 0, 0), p=4, S=50000)
```

```{r extended model Matrix B0}
#| label: table - Extended Model B0
#| tbl-cap: Mean of the Matrix B[0]
knitr::kable(round(apply(extended$B0.mean, 1:2, mean), 4))
```

```{r extended model Matrix B+}
#| label: table - Extended Model B+
#| tbl-cap: Mean of the Matrix B[+]
knitr::kable(round(apply(extended$B1.mean, 1:2, mean), 4))
```

Impulse response functions of the t-distributed innovation model show a positive monetary policy shock on variables. In the short run, interest rates immediately respond to the shock and normalised around five years after the shock. The consumer price index slightly decreases in the short run. There are no significant effects on GDP, stock prices, or stock price volatility. Consumer price index and exchange rates decrease slightly, but they are also not significant. 

```{r extended model irf, echo=TRUE}
#| code-fold: true
#| code-summary: "See R code"

# Define the lag order and the number of iterations
############################################################
p           <- 4          # number of lags (data frequency (quarter))
S           <- 50000      # number of posterior draws
h           <- 20         # forecast horizon

# Create Y and X matrices
############################################################
Y           <- variables[(p+1):nrow(variables), ]
X           <- matrix(1, nrow(Y), 1)
for (i in 1:p){
  X         <- cbind(X, variables[(p+1):nrow(variables)-i, ])
  }

T           <- nrow(Y)    # number of time periods
N           <- ncol(Y)    # number of variables
K           <- 1+p*N

# Convert it into a matrix form
Y           <- as.matrix(Y)
X           <- as.matrix(X)

# Calculate the MLE
############################################################
A.hat       <- solve(t(X)%*%X)%*%t(X)%*%Y                
Sigma.hat   <- t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

# Specify prior distribution parameters
############################################################
kappa.1     <- 1
kappa.2     <- 100

A.prior           <- matrix(0, nrow(A.hat), ncol(A.hat))
A.prior[2:(N+1),] <- diag(N)
V.prior           <- diag(c(kappa.2, kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior           <- diag(diag(Sigma.hat))
nu.prior          <- N+1
s.prior.lambda    <- 5   # assume that it is fixed
nu.prior.lambda   <- 5   # assume that it is fixed
lambda            <- s.prior.lambda/rchisq(1, nu.prior.lambda)

# Initialise arrays to store posterior draws
Sigma.posterior   <- array(NA, c(N,N,S))
A.posterior       <- array(NA, c(K,N,S))
lambda.posterior  <- rep(NA, S)
B0.posterior      <- array(NA, c(N,N,S))
B1.posterior      <- array(NA, c(N,K,S))

for (s in 1:S){
  
  # Specify the matrix normal-inverse Wishart posterior parameters
  ############################################################
  V.bar.inv  <- t(X)%*%X/lambda + diag(1/diag(V.prior))
  V.bar      <- solve(V.bar.inv)
  A.bar      <- V.bar%*%(t(X)%*%Y/lambda + diag(1/diag(V.prior))%*%A.prior)
  nu.bar     <- nrow(Y) + nu.prior
  S.bar      <- S.prior + t(Y)%*%Y/lambda + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
  S.bar.inv  <- solve(S.bar)
  
  # Draw Posterior distribution
  ############################################################
  ## Draw from the Reduced Form
  ### Draw Sigma from the inverse Wishart distribution
  Sigma.posterior.inv   <- rWishart(1, df=nu.bar, Sigma=S.bar.inv)[,,1]
  Sigma.posterior[,,s]  <- solve(Sigma.posterior.inv)
  
  ### Draw A from matrix-variate normal distribution
  A.posterior[,,s]      <- matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.bar), sigma=Sigma.posterior[,,s]%x%V.bar), ncol=N)
  
  ### Draw lambda from inverse gamma 2 distribution
  s.posterior.lambda    <- sum(diag(Sigma.posterior.inv%*%t(Y-X%*%A.posterior[,,s])%*%(Y-X%*%A.posterior[,,s]))) + s.prior.lambda
  nu.posterior.lambda   <- nrow(Y)*N + nu.prior.lambda
  lambda   <- s.posterior.lambda/rchisq(1, nu.posterior.lambda)
  lambda.posterior[s]   <- lambda
  
  ## Draw from the Structural Form
  ### Draw B0
  cholSigma.s        <- chol(Sigma.posterior[,,s])
  B0.posterior[,,s]  <- solve(t(cholSigma.s))

  ### Draw Bplus
  B1.posterior[,,s]  <- B0.posterior[,,s]%*%t(A.posterior[,,s])
  }
  
# Identification via sign restrictions 
############################################################
# Generate corresponding R matrix
sign.restrictions <- c(-1, 1, -1, 1, 0, 0)
R <- diag(sign.restrictions)
  
# Initialise arrays to store Q identified estimates
i.vec <- c()
Q.store      <- array(NA, c(N,N,S))
B0.store     <- array(NA, c(N,N,S))
B1.store     <- array(NA, c(N,K,S))
  
for (s in 1:S){
  B0.tilde      <- B0.posterior[,,s]
  B1.tilde      <- B1.posterior[,,s]
    
  sign.restrictions.do.not.hold = TRUE
  i=1
  while (sign.restrictions.do.not.hold){
    X           <- matrix(rnorm(N*N), N, N)         
    QR          <- qr(X, tol=1e-10)
    Q           <- qr.Q(QR, complete=TRUE)
    R           <- qr.R(QR, complete=TRUE)
    Q           <- t(Q %*% diag(sign(diag(R))))
    B0          <- Q%*%B0.tilde                    
    B1          <- Q%*%B1.tilde                   
    B0.inv      <- solve(B0)      
    check       <- all(B0.inv[1,2]<0, B0.inv[2,2]>0, B0.inv[3,2]<0, B0.inv[4,2]>0)
    
    if (check){sign.restrictions.do.not.hold = FALSE}
    i=i+1
    }
  
  i.vec <- c(i.vec, i) 
  Q.store[,,s]   <- Q
  B0.store[,,s]  <- B0
  B1.store[,,s]  <- B1
  }

# Impulse response functions
############################################################
# Transform B0 matrices to B
# Initialise arrays to store posterior draws
B.posterior <- array(NA, c(N,N,S))
  
for (s in 1:S){
  B.posterior[,,s]  <- solve(B0.store[,,s])
  }

# Initialise arrays to store posterior draws
IRF.posterior       <- array(NA, c(N,N,h+1,S))
IRF.inf.posterior   <- array(NA, c(N,N,S))
J                   <- cbind(diag(N), matrix(0,N,N*(p-1)))
  
for (s in 1:S){
  # Define A matrix in VAR(1) representation
  A.bold                     <- rbind(t(A.posterior[2:(1+N*p),,s]), cbind(diag(N*(p-1)), matrix(0,N*(p-1),N)))
  IRF.inf.posterior[,,s]     <- J%*%solve(diag(N*p)-A.bold)%*%t(J)%*%B.posterior[,,s]
  A.bold.power               <- A.bold
  
  for (i in 1:(h+1)){
    if (i==1){
      IRF.posterior[,,i,s]      <- B.posterior[,,s]
      } else {
        IRF.posterior[,,i,s]    <- J%*%A.bold.power%*%t(J)%*%B.posterior[,,s]
        A.bold.power            <- A.bold.power%*%A.bold
      }
    }
  }

library(HDInterval)

# Impulse response functions plots
############################################################
IRF.posterior.mps   <- IRF.posterior[,2,,]
IRFs.k1             <- apply(IRF.posterior.mps,1:2,median)
IRFs.inf.k1         <- apply(IRF.posterior.mps,1,mean)
rownames(IRFs.k1)   <- colnames(variables)

IRFs.k1.hdi    <- apply(IRF.posterior.mps, 1:2, hdi, credMass=0.68)
hh             <- 1:(h+1)

par(mfrow=c(3, 2), mar=c(3, 3, 2, 2), cex.axis=1.5, cex.lab=1.5)
for (n in 1:N){
  ylims=range(IRFs.k1[n,hh], IRFs.k1.hdi[,n,1:(h+1)], 0)
  plot(hh,IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="", main=rownames(IRFs.k1)[n])
  if (n==5){
    axis(1, c(1,5,9,13,17,21), c("0","1yr","2yr","3yr","4yr","5yr"))
    } else {
      axis(1, c(1,5,9,13,17,21), c("0","1yr","2yr","3yr","4yr","5yr"))
      }
  axis(2, c(ylims[1], 0, ylims[2]), round(c(ylims[1], 0, ylims[2]), 3))
  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh], IRFs.k1.hdi[2,n,(h+1):1]), col=mcxs2.shade1, border=mcxs2.shade1)
  abline(h=0)
  lines(hh, IRFs.k1[n,hh], lwd=2, col="darkblue")
  }
```

## 5.3 Bayesian SVAR Model with Common Stochastic Volatility




# 6. Conclusion


## References {.unnumbered}
